#!/usr/bin/env ruby

require 'bundler/setup'
require 'nokogiri'
require 'mechanize'
require 'set'
require 'date'

EXCLUDE_KEYWORDS = Set[
  'Hakin9',
  'KitPloit',
  'blockchain',
  'DeFi',
  'crypto',
  'AI',
  'ML'
]

Feed = Data.define(
  :type,
  :website_name,
  :website_url,
  :website_summary,
  :feed_url,
  :feed_type
)

@agent = Mechanize.new
@agent.user_agent_alias = 'Linux Firefox'

def normalize_url(url) = URI.parse(url).normalize.to_s

def resolve_url(url)
  # NOTE: we have to send a `GET` request because some web servers exclude
  # `HEAD` requests for some reason.
  #
  # BUG: Mechanize#head is sloooow for some reason
  @agent.get(url).uri
rescue => error
  warn "#{url}: #{error.message}"
end

def parse_html(text) = Nokogiri::HTML.fragment(text).inner_text

def parse_keywords(text) = text.split(/[\s[:punct:]]+/).to_set
def check_keywords(text) = parse_keywords(text).intersect?(EXCLUDE_KEYWORDS)

def parse_file
  section = nil

  File.open('allinfosecnews_sources.md') do |file|
    # I originally tried parsing the markdown with kramdown, but each_line with
    # a regex ended up being simpler.
    file.each_line(chomp: true) do |line|
      case line
      when '## InfoSec / Cybersecurity News'     then section = :news
      when '## InfoSec / Cybersecurity Podcasts' then section = :podcasts
      when '## InfoSec / Cybersecurity Videos'   then section = :videos
      when '## InfoSec / Cybersecurity Jobs'     then section = :jobs
      when '# all InfoSec News - Sources',
           '## Contents',
           "**[â¬† back to top](#contents)**",
           ''
        # no-op
      else
        if (match = line.match(/^-\s+\[(?<link_title>[^\]]+|[^\[]*\[[^\]]*\][^\]]*)\]\((?<link_url>[^\)]+)\)\s+-\s+(?:(?<description>.*)?\s+)?\(RSS feed: (?<feed_url>[^\)]+)\)$/))
          website_name    = match[:link_title]
          website_url     = match[:link_url]
          website_summary = parse_html(match[:description])
          feed_url        = match[:feed_url]
          feed_type       = if feed_url.end_with?('atom.xml')
                              :atom
                            else
                              :rss
                            end

          yield Feed.new(
            section,
            website_name,
            website_url,
            website_summary,
            feed_url,
            feed_type
          )
        end
      end
    end
  end
end

def filter_feeds
  parse_file do |feed|
    next unless feed.type == :news # we only care about news feeds

    next if check_keywords(feed.website_name) ||
            check_keywords(feed.website_summary)

    yield feed
  end
end

def normalize_feeds
  filter_feeds do |feed|
    if (resolved_feed_url = resolve_url(feed.feed_url))
      # NOTE: attempt to resolve the website URL, but fall back to the
      # normalized version
      resolved_website_url = resolve_url(feed.website_url) || \
                             normalize_url(feed.website_url)

      yield Feed.new(
        type:            feed.type,
        website_name:    feed.website_name,
        website_url:     resolved_website_url,
        website_summary: feed.website_summary,
        feed_url:        resolved_feed_url,
        feed_type:       feed.feed_type
      )
    end
  end
end

opml = Nokogiri::XML::Builder.new do |xml|
  xml.opml(version: '2.0') {
    xml.body {
      normalize_feeds do |feed|
        xml.outline(
          category: 'InfoSec',
          htmlUrl:  feed.website_url,
          text:     feed.website_name,
          title:    feed.website_name,
          type:     feed.feed_type,
          xmlUrl:   feed.feed_url
        )
      end
    }
  }
end

File.write('allinfosecnews.opml',opml.to_xml)
